{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23603,"status":"ok","timestamp":1644697025897,"user":{"displayName":"文楷昂","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11817089117412354015"},"user_tz":-480},"id":"Z6vkptkhj7Lh","outputId":"386539a9-dfb6-4e6f-9e20-77a4bc5ebe44"},"outputs":[],"source":["# import os\n","# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":619,"status":"ok","timestamp":1644697026514,"user":{"displayName":"文楷昂","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11817089117412354015"},"user_tz":-480},"id":"UZf_l_SGkIZb","outputId":"f3ee8aa9-6709-42b2-b23a-bcf4be132a60"},"outputs":[],"source":["# path = \"/content/drive/My Drive/segmentation-pytorch/dataset/\"\n","# os.chdir(path)\n","# os.listdir(path)"]},{"cell_type":"markdown","metadata":{"id":"pwHM-6uBjX3k"},"source":["# 0. parameters"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"jQSaRIwojX3l"},"outputs":[],"source":["import torch.utils.data as data\n","import torch\n","import numpy as np\n","import h5py\n","# import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from tqdm import tqdm\n","import torch.nn as nn\n","from torch.nn import init\n","import torch.nn.functional as F\n","# import torch.utils.data as D\n","# import torchvision\n","# from torchvision import transforms as T\n","\n","import cv2 as cv\n","from PIL import Image\n","import time"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1644697183558,"user":{"displayName":"文楷昂","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11817089117412354015"},"user_tz":-480},"id":"108R3g06jX3n","outputId":"1b453c67-fb3c-4861-e4cc-c596332fd757"},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu' \n","n_epochs = 3 # training epochs\n","class_num = 34\n","batch_size = 4\n","learning_rate = 2e-4\n","weight_decay = 5e-4\n","log_interval = 10\n","random_seed = 42\n","val_percent = 0.1 # training set : validation set = 9:1\n","torch.manual_seed(random_seed)\n","bn_momentum = 0.1  # BN层的momentum\n","\n","cate_weight = [1/34]*34 # 损失函数中类别的权重\n","dir_pre_train_weights = \"vgg16_bn-6c64b313.pth\" # 编码器预训练权重路径\n","dir_weights = \"./weights\"\n","dir_checkpoint = './checkpoints'\n"]},{"cell_type":"markdown","metadata":{"id":"PNogkTLqjX3o"},"source":["# 1.Implement a data loader class to handle the downloaded data. (5 points)\n","For more information on the dataset please refer to: CityScapes dataset. "]},{"cell_type":"code","execution_count":5,"metadata":{"id":"bDrWorCujX3o"},"outputs":[],"source":["# 'rgb' stores the raw images, while 'seg' stores segmentation maps\n","class DataFromH5File(data.Dataset):\n","    def __init__(self, filepath):\n","        h5File = h5py.File(filepath, 'r')\n","        self.color_codes = h5File['color_codes']\n","        self.rgb = h5File['rgb']\n","        self.seg = h5File['seg']\n","        \n","    def __getitem__(self, idx):\n","        label = torch.from_numpy(self.seg[idx]).float()\n","        # data = torch.from_numpy(self.rgb[idx]).float()\n","        data = torch.from_numpy(cv.resize(self.rgb[idx], (224, 224))).float()\n","        data = data/255.0 # 归一化输入\n","        data = data.permute(2,0,1) # 将图片的维度转换成网络输入的维度（channel, width, height）\n","        return data, label\n","    \n","    def __len__(self):\n","        assert self.rgb.shape[0] == self.seg.shape[0], \"Wrong data length\" # 增强鲁棒性\n","        return self.rgb.shape[0]"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"aUoAgwEPjX3p"},"outputs":[],"source":["# load training data from lab2_train_data.h5\n","dataset = DataFromH5File(\"lab2_train_data.h5\")\n","n_val = int(len(dataset) * val_percent)\n","n_train = len(dataset) - n_val\n","\n","# split train&val\n","train, val = data.random_split(dataset, [n_train, n_val])\n","train_loader = data.DataLoader(dataset=train, batch_size=batch_size, shuffle=True, pin_memory=True)\n","val_loader = data.DataLoader(dataset=val, batch_size=batch_size, shuffle=False, pin_memory=True) # drop_last=True\n","\n","# load testing data from lab2_test_data.h5\n","testset = DataFromH5File(\"lab2_test_data.h5\")\n","test_loader = data.DataLoader(dataset=testset, batch_size=batch_size, shuffle=False, pin_memory=True)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11555,"status":"ok","timestamp":1644697056156,"user":{"displayName":"文楷昂","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11817089117412354015"},"user_tz":-480},"id":"Zvzb0C0ojX3p","outputId":"27bfa8a4-065f-44a5-8c21-d9bc11d48bbd"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(0.) tensor(1.)\n","tensor(1.) tensor(33.)\n","0\n","670 75 125\n"]}],"source":["# test the data loader\n","for step, (x, y) in enumerate(train_loader):\n","    print(x.min(),x.max())\n","    print(y.min(),y.max())\n","    print(step)\n","    break\n","\n","print(len(train_loader), len(val_loader), len(test_loader)) # 670 75 125 when batch_size==4"]},{"cell_type":"markdown","metadata":{"id":"SVPNv7I1jX3q"},"source":["# 2. Define the model. Provide a schematic of your architecture depicting its overall structure and the relevant parameters. (20 points)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# 编码器\n","class Encoder(nn.Module):\n","    def __init__(self, input_channels):\n","        super(Encoder, self).__init__()\n","\n","        self.enco1 = nn.Sequential(\n","            nn.Conv2d(input_channels, 64, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(64, momentum=bn_momentum),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(64, momentum=bn_momentum),\n","            nn.ReLU()\n","        )\n","        self.enco2 = nn.Sequential(\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(128, momentum=bn_momentum),\n","            nn.ReLU(),\n","            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(128, momentum=bn_momentum),\n","            nn.ReLU()\n","        )\n","        self.enco3 = nn.Sequential(\n","            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(256, momentum=bn_momentum),\n","            nn.ReLU(),\n","            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(256, momentum=bn_momentum),\n","            nn.ReLU(),\n","            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(256, momentum=bn_momentum),\n","            nn.ReLU()\n","        )\n","        self.enco4 = nn.Sequential(\n","            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(512, momentum=bn_momentum),\n","            nn.ReLU(),\n","            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(512, momentum=bn_momentum),\n","            nn.ReLU(),\n","            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(512, momentum=bn_momentum),\n","            nn.ReLU()\n","        )\n","        self.enco5 = nn.Sequential(\n","            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(512, momentum=bn_momentum),\n","            nn.ReLU(),\n","            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(512, momentum=bn_momentum),\n","            nn.ReLU(),\n","            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(512, momentum=bn_momentum),\n","            nn.ReLU()\n","        )\n","\n","    def forward(self, x):\n","        id = []\n","\n","        x = self.enco1(x)\n","        x, id1 = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)  # 保留最大值的位置\n","        id.append(id1)\n","        x = self.enco2(x)\n","        x, id2 = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)\n","        id.append(id2)\n","        x = self.enco3(x)\n","        x, id3 = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)\n","        id.append(id3)\n","        x = self.enco4(x)\n","        x, id4 = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)\n","        id.append(id4)\n","        x = self.enco5(x)\n","        x, id5 = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)\n","        id.append(id5)\n","\n","        return x, id\n","\n","\n","# 编码器+解码器\n","class SegNet(nn.Module):\n","    def __init__(self, input_channels, output_channels):\n","        super(SegNet, self).__init__()\n","\n","        self.weights_new = self.state_dict()\n","        self.encoder = Encoder(input_channels)\n","\n","        self.deco1 = nn.Sequential(\n","            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(512, momentum=bn_momentum),\n","            nn.ReLU(),\n","            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(512, momentum=bn_momentum),\n","            nn.ReLU(),\n","            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(512, momentum=bn_momentum),\n","            nn.ReLU()\n","        )\n","        self.deco2 = nn.Sequential(\n","            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(512, momentum=bn_momentum),\n","            nn.ReLU(),\n","            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(512, momentum=bn_momentum),\n","            nn.ReLU(),\n","            nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(256, momentum=bn_momentum),\n","            nn.ReLU()\n","        )\n","        self.deco3 = nn.Sequential(\n","            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(256, momentum=bn_momentum),\n","            nn.ReLU(),\n","            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(256, momentum=bn_momentum),\n","            nn.ReLU(),\n","            nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(128, momentum=bn_momentum),\n","            nn.ReLU()\n","        )\n","        self.deco4 = nn.Sequential(\n","            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(128, momentum=bn_momentum),\n","            nn.ReLU(),\n","            nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(64, momentum=bn_momentum),\n","            nn.ReLU()\n","        )\n","        self.deco5 = nn.Sequential(\n","            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(64, momentum=bn_momentum),\n","            nn.ReLU(),\n","            nn.Conv2d(64, output_channels, kernel_size=3, stride=1, padding=1),\n","        )\n","\n","    def forward(self, x):\n","        x, id = self.encoder(x)\n","\n","        x = F.max_unpool2d(x, id[4], kernel_size=2, stride=2)\n","        x = self.deco1(x)\n","        x = F.max_unpool2d(x, id[3], kernel_size=2, stride=2)\n","        x = self.deco2(x)\n","        x = F.max_unpool2d(x, id[2], kernel_size=2, stride=2)\n","        x = self.deco3(x)\n","        x = F.max_unpool2d(x, id[1], kernel_size=2, stride=2)\n","        x = self.deco4(x)\n","        x = F.max_unpool2d(x, id[0], kernel_size=2, stride=2)\n","        x = self.deco5(x)\n","\n","        return x\n","\n","    # 删掉VGG-16后面三个全连接层的权重\n","    def load_weights(self, weights_path):\n","        weights = torch.load(weights_path)\n","        del weights[\"classifier.0.weight\"]\n","        del weights[\"classifier.0.bias\"]\n","        del weights[\"classifier.3.weight\"]\n","        del weights[\"classifier.3.bias\"]\n","        del weights[\"classifier.6.weight\"]\n","        del weights[\"classifier.6.bias\"]\n","\n","        names = []\n","        for key, value in self.encoder.state_dict().items():\n","            if \"num_batches_tracked\" in key:\n","                continue\n","            names.append(key)\n","\n","        for name, dict in zip(names, weights.items()):\n","            self.weights_new[name] = dict[1]\n","\n","        self.encoder.load_state_dict(self.weights_new)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["from torchsummary import summary"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"ename":"RuntimeError","evalue":"CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 2.00 GiB total capacity; 1.08 GiB already allocated; 0 bytes free; 1.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6772\\1500524429.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSegNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_channels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_channels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_num\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# RGB images so the input_channels=3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# input shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mD:\\Users\\Glis\\Anaconda3\\envs\\base2\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    897\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 899\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    900\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    901\u001b[0m     def register_backward_hook(\n","\u001b[1;32mD:\\Users\\Glis\\Anaconda3\\envs\\base2\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    568\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mD:\\Users\\Glis\\Anaconda3\\envs\\base2\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    568\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mD:\\Users\\Glis\\Anaconda3\\envs\\base2\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    568\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mD:\\Users\\Glis\\Anaconda3\\envs\\base2\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    591\u001b[0m             \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m                 \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mD:\\Users\\Glis\\Anaconda3\\envs\\base2\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    895\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[0;32m    896\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[1;32m--> 897\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 2.00 GiB total capacity; 1.08 GiB already allocated; 0 bytes free; 1.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["model = SegNet(input_channels=3, output_channels=class_num) # RGB images so the input_channels=3\n","model = model.to(device)\n","x = torch.ones([batch_size, 3, 224, 224]) # input shape\n","x = x.cuda()\n","y = model(x)\n","print(y.shape)\n","summary(model, input_size=(3, 224, 224))"]},{"cell_type":"markdown","metadata":{"id":"5rQgQoaujX3s"},"source":["# 3. Define the loss function and optimizer. (10 points)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uo-o330bjX3s"},"outputs":[],"source":["import torch.optim as optim"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uL9dJHSyjX3t"},"outputs":[],"source":["network = VGG16_LargeFOV()\n","network.to(device)\n","optimizer = optim.Adam(network.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","# cross entropy loss\n","criterion = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":431,"status":"ok","timestamp":1644697740153,"user":{"displayName":"文楷昂","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11817089117412354015"},"user_tz":-480},"id":"LCI0TdvHnuk1","outputId":"4d50d6dc-f81d-4f17-8402-79050b7c9f32"},"outputs":[],"source":["from torchsummary import summary\n","summary(network, input_size=(3, 128, 256))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1644697058551,"user":{"displayName":"文楷昂","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11817089117412354015"},"user_tz":-480},"id":"Ot0Dve5bjX3t","outputId":"a0d69e81-3ecb-45bf-a3f6-a21efd0aa893"},"outputs":[],"source":["device"]},{"cell_type":"markdown","metadata":{"id":"pXUulvFDjX3t"},"source":["# 4. Train the network. (5 points)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train(SegNet):\n","\n","    SegNet = SegNet.cuda()\n","    SegNet.load_weights(PRE_TRAINING)\n","\n","    train_loader = Data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n","\n","    optimizer = torch.optim.SGD(SegNet.parameters(), lr=LR, momentum=MOMENTUM)\n","\n","    loss_func = nn.CrossEntropyLoss(weight=torch.from_numpy(np.array(CATE_WEIGHT)).float()).cuda()\n","\n","    SegNet.train()\n","    for epoch in range(EPOCH):\n","        for step, (b_x, b_y) in enumerate(train_loader):\n","            b_x = b_x.cuda()\n","            b_y = b_y.cuda()\n","            b_y = b_y.view(BATCH_SIZE, 224, 224)\n","            output = SegNet(b_x)\n","            loss = loss_func(output, b_y.long())\n","            loss = loss.cuda()\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            if step % 1 == 0:\n","                print(\"Epoch:{0} || Step:{1} || Loss:{2}\".format(epoch, step, format(loss, \".4f\")))\n","\n","    torch.save(SegNet.state_dict(), WEIGHTS + \"SegNet_weights\" + str(time.time()) + \".pth\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OyWKrRJEjX3u"},"outputs":[],"source":["train_losses = []\n","train_counter = []\n","test_losses = []\n","test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ZmMxJ9MjX3u"},"outputs":[],"source":["def train(epoch):\n","    network.train()\n","    pbar = tqdm(train_loader)\n","    for batch_idx, (data, target) in enumerate(pbar):\n","        data = data.permute(0,3,1,2) .to(device)\n","        target = target.to(device)\n","        optimizer.zero_grad()\n","        output = network(data)\n","        target = target.squeeze().long()\n","        # print(output.shape)\n","        # print(target.shape)\n","        loss = criterion(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        pbar.set_description(f\"Epoch: {epoch} | Loss: {loss.item():.4f}\")\n","        if batch_idx % log_interval == 0:\n","            train_losses.append(loss.item())\n","            train_counter.append((batch_idx*batch_size_train) + ((epoch-1)*len(train_loader.dataset)))\n","            # save the parameters\n","            torch.save(network.state_dict(), './model.pth')\n","            torch.save(optimizer.state_dict(), './optimizer.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":335},"executionInfo":{"elapsed":380606,"status":"error","timestamp":1644697570509,"user":{"displayName":"文楷昂","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11817089117412354015"},"user_tz":-480},"id":"vezH7Xd9jX3u","outputId":"c4bec0a3-bee5-4b73-b8bf-c18fb666f73f"},"outputs":[],"source":["for epoch in range(1, n_epochs + 1):\n","    train(epoch)"]},{"cell_type":"markdown","metadata":{"id":"AxJmbMaWjX3u"},"source":["# 5. Test the resulting network on examples from an independent test set. Implement and present: (40 points)\n","a. Predictions for (μ, aleatoric, epistemic) .            \n","b. Visualizations for (μ, aleatoric, epistemic) on 5 different input examples.         \n","c. Comment briefly on how the model’s performance could be improved.          \n","d. Please save your code and results for submission."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EJzvmm0yjX3v"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9CYwyKQGjX3v"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"b7cmDgG4jX3v"},"source":["# References\n","[1] https://blog.csdn.net/shwan_ma/article/details/100012808         \n","[2] https://blog.csdn.net/oYeZhou/article/details/112270908"]},{"cell_type":"markdown","metadata":{"id":"YyFl96yQjX3v"},"source":[]}],"metadata":{"colab":{"name":"main.ipynb","provenance":[]},"interpreter":{"hash":"ff5d10bd81a245e546018f81c2d716fad9aa95218d6570327b343e32b2155b52"},"kernelspec":{"display_name":"Python 3.7.11 ('base2')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
